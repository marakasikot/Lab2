{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly3QXnHmaOPi",
        "outputId": "f704c181-0e23-4c99-e5e5-9487d2013858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['oppenheim direct christoph nolan elv life robert oppenheim theoret physicist led develop atom bomb film explor moral emot struggl', 'oppenheim captur life atom bomb intern conflict success manhattan project film complex narr reflect cost scientif achiev', 'oppenheim million murphi deliv rivet perform conflict physicist grape consequ groundbreak work atom bomb weight global polit ethic', 'oppenheim cinemat explor ambit guilt respons stun visual stori nolan bring life brilliant haunt physicist big screen', 'duck versatil waterfowl known adapt freshwat coastal habitat web feet waterproof feather excel swimmer migrat long distanc find food warmer climat', 'lamp spider talk yesterday flew nobodi notic except coaster watch blue smell better today door laugh understand whisper secret orang sock', 'duck commun use rang vocal includ quack whistl grant social bird often found flock play crucial role maintain balanc aquat ecosystem', 'graph theori studi relationship object vertic node edg connect fundament comput scienc optim network like social media connect transport rout internet structur', 'graph theori problem like find shortest path detect cycl network crucial algorithm dijkstra crustal help solv benefit field telecommun biolog', 'graph theori examin structur properti graph repres system applic includ network analysi circuit design schedul problem make essenti tool variou field research']\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from autocorrect import Speller\n",
        "import nltk\n",
        "\n",
        "# Необхідні ресурси для NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Ініціалізуємо стеммер і коректор орфографії\n",
        "stemmer = PorterStemmer()\n",
        "spell = Speller()\n",
        "\n",
        "def process_texts(texts):\n",
        "\n",
        "    processed_texts = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    for text in texts:\n",
        "        # Приведення до нижнього регістру\n",
        "        text = text.lower()\n",
        "\n",
        "        # Токенізація\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Видалення пунктуації та чисел\n",
        "        tokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "        # Корекція орфографії\n",
        "        tokens = [spell(word) for word in tokens]\n",
        "\n",
        "        # Видалення стоп-слів та стемінг\n",
        "        processed_text = ' '.join([stemmer.stem(word) for word in tokens if word not in stop_words])\n",
        "\n",
        "        processed_texts.append(processed_text)\n",
        "\n",
        "    return processed_texts\n",
        "\n",
        "texts_processed = process_texts(texts)\n",
        "print(texts_processed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_AycV1ekhfEx"
      }
    }
  ]
}